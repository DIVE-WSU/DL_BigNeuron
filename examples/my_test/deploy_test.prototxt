name: "BIG_N_TEST"
input: "data"
input_shape{ 
dim:1
dim: 1
dim:  640
dim:  640
dim:  29
}

layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 64
    kernel_size: 7
	kernel_size: 7
	kernel_size: 4
    stride: 2
	pad:3
	pad:3
	pad:0
  }
  phase:PREDICT
}

layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}

layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_shape: 3
	kernel_shape: 3
	kernel_shape: 3
    stride_shape: 2
	stride_shape: 2
	stride_shape: 2
  }
}



#------------------------------------------- conv2x-------------------------------------
#---------------con2x_top-------------

layer {
  name: "conv2x_top"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2x_top"
  phase:PREDICT
  convolution_param {
    num_output: 256
    kernel_size: 1
  }
}

#---------------------------  block layers conv2_1 ----------------------------

layer {
  name: "conv2x_1_1"
  type: "Convolution"
  bottom: "conv2x_top"
  top: "conv2x_1_1"
phase:PREDICT
  convolution_param {
    num_output: 64
    kernel_size: 1
  }
}

layer {
  name: "relu2x_1_1"
  type: "ReLU"
  bottom: "conv2x_1_1"
  top: "conv2x_1_1"
}
layer {
  name: "conv2x_1_2"
  type: "Convolution"
  bottom: "conv2x_1_1"
  top: "conv2x_1_2"
phase:PREDICT
  convolution_param {
    num_output: 64
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad:1
	pad:1
	pad:0
  }
}

layer {
  name: "relu2x_1_2"
  type: "ReLU"
  bottom: "conv2x_1_2"
  top: "conv2x_1_2"
}
layer {
  name: "conv2x_1_3"
  type: "Convolution"
  bottom: "conv2x_1_2"
  top: "conv2x_1_3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  phase:PREDICT
  convolution_param {
    num_output: 256
	kernel_size: 1
  }
}

layer{
name: "conv2x_1_sum"
type:  "Eltwise"
bottom:"conv2x_top"
bottom:"conv2x_1_3"
top:"conv2x_1_sum"
eltwise_param{
operation: SUM
}
}

layer {
  name: "relu2x_1_sum"
  type: "ReLU"
  bottom: "conv2x_1_sum"
  top: "conv2x_1_sum"
}



#------------ block layers conv2_2--------------------

layer {
  name: "conv2x_2_1"
  type: "Convolution"
  bottom: "conv2x_1_sum"
  top: "conv2x_2_1"
phase:PREDICT
  convolution_param {
    num_output: 64
    kernel_size: 1
  }
}

layer {
  name: "relu2x_2_1"
  type: "ReLU"
  bottom: "conv2x_2_1"
  top: "conv2x_2_1"
}
layer {
  name: "conv2x_2_2"
  type: "Convolution"
  bottom: "conv2x_2_1"
  top: "conv2x_2_2"
phase:PREDICT
  convolution_param {
    num_output: 64
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad:1
	pad:1
	pad:0
  }
}

layer {
  name: "relu2x_2_2"
  type: "ReLU"
  bottom: "conv2x_2_2"
  top: "conv2x_2_2"
}
layer {
  name: "conv2x_2_3"
  type: "Convolution"
  bottom: "conv2x_2_2"
  top: "conv2x_2_3"
phase:PREDICT
  convolution_param {
    num_output: 256
	kernel_size: 1
  }
}

layer{
name: "conv2x_2_sum"
type:  "Eltwise"
bottom:"conv2x_1_sum"
bottom:"conv2x_2_3"
top:"conv2x_2_sum"
eltwise_param{
operation: SUM
}
}

layer {
  name: "relu2x_2_sum"
  type: "ReLU"
  bottom: "conv2x_2_sum"
  top: "conv2x_2_sum"
}
#------------ block layers conv2_3--------------------
layer {
  name: "conv2x_3_1"
  type: "Convolution"
  bottom: "conv2x_2_sum"
  top: "conv2x_3_1"
phase:PREDICT
  convolution_param {
    num_output: 64
    kernel_size: 1
  }
}

layer {
  name: "relu2x_3_1"
  type: "ReLU"
  bottom: "conv2x_3_1"
  top: "conv2x_3_1"
}
layer {
  name: "conv2x_3_2"
  type: "Convolution"
  bottom: "conv2x_3_1"
  top: "conv2x_3_2"
phase:PREDICT
  convolution_param {
    num_output: 64
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad:1
	pad:1
	pad:0
  }
}

layer {
  name: "relu2x_3_2"
  type: "ReLU"
  bottom: "conv2x_3_2"
  top: "conv2x_3_2"
}
layer {
  name: "conv2x_3_3"
  type: "Convolution"
  bottom: "conv2x_3_2"
  top: "conv2x_3_3"
  phase:PREDICT
  convolution_param {
    num_output: 256
	kernel_size: 1
  }
}

layer{
name: "conv2x_3_sum"
type:  "Eltwise"
bottom:"conv2x_2_sum"
bottom:"conv2x_3_3"
top:"conv2x_3_sum"
eltwise_param{
operation: SUM
}
}

layer {
  name: "relu2x_3_sum"
  type: "ReLU"
  bottom: "conv2x_3_sum"
  top: "conv2x_3_sum"
  }
  
#------------ block layers conv2_3-------------------- 


#------------------- conv3_x_-----------------------
layer {
  name: "conv3x_top"
  type: "Convolution"
  bottom: "conv2x_3_sum"
  top: "conv3x_top"
phase:PREDICT
  convolution_param {
    num_output: 512
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	stride :2
	stride :2
	stride :1
	pad :1
	pad :1
	pad :0
  }
}

#------------ block layers conv3_1-------------------- 
layer {
  name: "conv3x_1_1"
  type: "Convolution"
  bottom: "conv3x_top"
  top: "conv3x_1_1"
phase:PREDICT
  convolution_param {
    num_output: 128
    kernel_size: 1
  }
}

layer {
  name: "relu3x_1_1"
  type: "ReLU"
  bottom: "conv3x_1_1"
  top: "conv3x_1_1"
}
layer {
  name: "conv3x_1_2"
  type: "Convolution"
  bottom: "conv3x_1_1"
  top: "conv3x_1_2"
phase:PREDICT
  convolution_param {
    num_output: 128
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad:1
	pad:1
	pad:0
  }
}

layer {
  name: "relu3x_1_2"
  type: "ReLU"
  bottom: "conv3x_1_2"
  top: "conv3x_1_2"
}
layer {
  name: "conv3x_1_3"
  type: "Convolution"
  bottom: "conv3x_1_2"
  top: "conv3x_1_3"
phase:PREDICT
  convolution_param {
    num_output: 512
	kernel_size: 1
  }
}

layer{
name: "conv3x_1_sum"
type:  "Eltwise"
bottom:"conv3x_top"
bottom:"conv3x_1_3"
top:"conv3x_1_sum"
eltwise_param{
operation: SUM
}
}

layer {
  name: "relu3x_1_sum"
  type: "ReLU"
  bottom: "conv3x_1_sum"
  top: "conv3x_1_sum"
  }
#------------ block layers conv3_2-------------------- 
 
  
layer {
  name: "conv3x_2_1"
  type: "Convolution"
  bottom: "conv3x_1_sum"
  top: "conv3x_2_1"
phase:PREDICT
  convolution_param {
    num_output: 128
    kernel_size: 1
  }
}

layer {
  name: "relu3x_2_1"
  type: "ReLU"
  bottom: "conv3x_2_1"
  top: "conv3x_2_1"
}
layer {
  name: "conv3x_2_2"
  type: "Convolution"
  bottom: "conv3x_2_1"
  top: "conv3x_2_2"
phase:PREDICT
  convolution_param {
    num_output: 128
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad:1
	pad:1
	pad:0
  }
}

layer {
  name: "relu3x_2_2"
  type: "ReLU"
  bottom: "conv3x_2_2"
  top: "conv3x_2_2"
}
layer {
  name: "conv3x_2_3"
  type: "Convolution"
  bottom: "conv3x_2_2"
  top: "conv3x_2_3"
phase:PREDICT
  convolution_param {
    num_output: 512
	kernel_size: 1
  }
}

layer{
name: "conv3x_2_sum"
type:  "Eltwise"
bottom:"conv3x_1_sum"
bottom:"conv3x_2_3"
top:"conv3x_2_sum"
eltwise_param{
operation: SUM
}
}

layer {
  name: "relu3x_2_sum"
  type: "ReLU"
  bottom: "conv3x_2_sum"
  top: "conv3x_2_sum"
  }

#------------ block layers conv3_3-------------------- 
layer {
  name: "conv3x_3_1"
  type: "Convolution"
  bottom: "conv3x_2_sum"
  top: "conv3x_3_1"
phase:PREDICT
  convolution_param {
    num_output: 128
    kernel_size: 1
  }
}

layer {
  name: "relu3x_3_1"
  type: "ReLU"
  bottom: "conv3x_3_1"
  top: "conv3x_3_1"
}
layer {
  name: "conv3x_3_2"
  type: "Convolution"
  bottom: "conv3x_3_1"
  top: "conv3x_3_2"
phase:PREDICT
  convolution_param {
    num_output: 128
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad:1
	pad:1
	pad:0
  }
}

layer {
  name: "relu3x_3_2"
  type: "ReLU"
  bottom: "conv3x_3_2"
  top: "conv3x_3_2"
}
layer {
  name: "conv3x_3_3"
  type: "Convolution"
  bottom: "conv3x_3_2"
  top: "conv3x_3_3"
phase:PREDICT
  convolution_param {
    num_output: 512
	kernel_size: 1
  }
}

layer{
name: "conv3x_3_sum"
type:  "Eltwise"
bottom:"conv3x_2_sum"
bottom:"conv3x_3_3"
top:"conv3x_3_sum"
eltwise_param{
operation: SUM
}
}

layer {
  name: "relu3x_3_sum"
  type: "ReLU"
  bottom: "conv3x_3_sum"
  top: "conv3x_3_sum"
  }
 
 # ----conv_4x_-----------------------------------------------
 
 layer {
  name: "conv4x_top"
  type: "Convolution"
  bottom: "conv3x_3_sum"
  top: "conv4x_top"
phase:PREDICT
  convolution_param {
    num_output: 1024
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	stride :2
	stride :2
	stride :1
	pad :1
	pad :1
	pad :0
  }
}
 #------------ block layers conv4_1-------------------- 
layer {
  name: "conv4x_1_1"
  type: "Convolution"
  bottom: "conv4x_top"
  top: "conv4x_1_1"
phase:PREDICT
  convolution_param {
    num_output: 256
    kernel_size: 1
  }
}

layer {
  name: "relu4x_1_1"
  type: "ReLU"
  bottom: "conv4x_1_1"
  top: "conv4x_1_1"
}
layer {
  name: "conv4x_1_2"
  type: "Convolution"
  bottom: "conv4x_1_1"
  top: "conv4x_1_2"
phase:PREDICT
  convolution_param {
    num_output: 256
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad:1
	pad:1
	pad:0
  }
}

layer {
  name: "relu4x_1_2"
  type: "ReLU"
  bottom: "conv4x_1_2"
  top: "conv4x_1_2"
}
layer {
  name: "conv4x_1_3"
  type: "Convolution"
  bottom: "conv4x_1_2"
  top: "conv4x_1_3"
phase:PREDICT
  convolution_param {
    num_output: 1024
	kernel_size: 1
  }
}

layer{
name: "conv4x_1_sum"
type:  "Eltwise"
bottom:"conv4x_top"
bottom:"conv4x_1_3"
top:"conv4x_1_sum"
eltwise_param{
operation: SUM
}
}

layer {
  name: "relu4x_1_sum"
  type: "ReLU"
  bottom: "conv4x_1_sum"
  top: "conv4x_1_sum"
  }
#------------ block layers conv4_2-------------------- 
 
  
layer {
  name: "conv4x_2_1"
  type: "Convolution"
  bottom: "conv4x_1_sum"
  top: "conv4x_2_1"
phase:PREDICT
  convolution_param {
    num_output: 256
    kernel_size: 1
  }
}

layer {
  name: "relu4x_2_1"
  type: "ReLU"
  bottom: "conv4x_2_1"
  top: "conv4x_2_1"
}
layer {
  name: "conv4x_2_2"
  type: "Convolution"
  bottom: "conv4x_2_1"
  top: "conv4x_2_2"
phase:PREDICT
  convolution_param {
    num_output: 256
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad:1
	pad:1
	pad:0
  }
}

layer {
  name: "relu4x_2_2"
  type: "ReLU"
  bottom: "conv4x_2_2"
  top: "conv4x_2_2"
}
layer {
  name: "conv4x_2_3"
  type: "Convolution"
  bottom: "conv4x_2_2"
  top: "conv4x_2_3"
phase:PREDICT
  convolution_param {
    num_output: 1024
	kernel_size: 1
  }
}

layer{
name: "conv4x_2_sum"
type:  "Eltwise"
bottom:"conv4x_1_sum"
bottom:"conv4x_2_3"
top:"conv4x_2_sum"
eltwise_param{
operation: SUM
}
}

layer {
  name: "relu4x_2_sum"
  type: "ReLU"
  bottom: "conv4x_2_sum"
  top: "conv4x_2_sum"
  }

#------------ block layers conv4_3-------------------- 
layer {
  name: "conv4x_3_1"
  type: "Convolution"
  bottom: "conv4x_2_sum"
  top: "conv4x_3_1"
phase:PREDICT
  convolution_param {
    num_output: 256
    kernel_size: 1
  }
}

layer {
  name: "relu4x_3_1"
  type: "ReLU"
  bottom: "conv4x_3_1"
  top: "conv4x_3_1"
}
layer {
  name: "conv4x_3_2"
  type: "Convolution"
  bottom: "conv4x_3_1"
  top: "conv4x_3_2"
phase:PREDICT
  convolution_param {
    num_output: 256
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad:1
	pad:1
	pad:0
  }
}

layer {
  name: "relu4x_3_2"
  type: "ReLU"
  bottom: "conv4x_3_2"
  top: "conv4x_3_2"
}
layer {
  name: "conv4x_3_3"
  type: "Convolution"
  bottom: "conv4x_3_2"
  top: "conv4x_3_3"
phase:PREDICT
  convolution_param {
    num_output: 1024
  }
}

layer{
name: "conv4x_3_sum"
type:  "Eltwise"
bottom:"conv4x_2_sum"
bottom:"conv4x_3_3"
top:"conv4x_3_sum"
eltwise_param{
operation: SUM
}
}

layer {
  name: "relu4x_3_sum"
  type: "ReLU"
  bottom: "conv4x_3_sum"
  top: "conv4x_3_sum"
  }
  # --------------block layers_4_4_-------------------------
  layer {
  name: "conv4x_4_1"
  type: "Convolution"
  bottom: "conv4x_3_sum"
  top: "conv4x_4_1"
phase:PREDICT
  convolution_param {
    num_output: 256
    kernel_size: 1
  }
}

layer {
  name: "relu4x_4_1"
  type: "ReLU"
  bottom: "conv4x_4_1"
  top: "conv4x_4_1"
}
layer {
  name: "conv4x_4_2"
  type: "Convolution"
  bottom: "conv4x_4_1"
  top: "conv4x_4_2"
phase:PREDICT
  convolution_param {
    num_output: 256
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad:1
	pad:1
	pad:0
  }
}

layer {
  name: "relu4x_4_2"
  type: "ReLU"
  bottom: "conv4x_4_2"
  top: "conv4x_4_2"
}
layer {
  name: "conv4x_4_3"
  type: "Convolution"
  bottom: "conv4x_4_2"
  top: "conv4x_4_3"
phase:PREDICT
  convolution_param {
    num_output: 1024
	kernel_size: 1
  }
}

layer{
name: "conv4x_4_sum"
type:  "Eltwise"
bottom:"conv4x_3_sum"
bottom:"conv4x_4_3"
top:"conv4x_4_sum"
eltwise_param{
operation: SUM
}
}

layer {
  name: "relu4x_4_sum"
  type: "ReLU"
  bottom: "conv4x_4_sum"
  top: "conv4x_4_sum"
  }
  
 #----------block layers 4x_5-------------------------------
 layer {
  name: "conv4x_5_1"
  type: "Convolution"
  bottom: "conv4x_4_sum"
  top: "conv4x_5_1"
phase:PREDICT
  convolution_param {
    num_output: 256
    kernel_size: 1
  }
}

layer {
  name: "relu4x_5_1"
  type: "ReLU"
  bottom: "conv4x_5_1"
  top: "conv4x_5_1"
}
layer {
  name: "conv4x_5_2"
  type: "Convolution"
  bottom: "conv4x_5_1"
  top: "conv4x_5_2"
phase:PREDICT
  convolution_param {
    num_output: 256
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad:1
	pad:1
	pad:0
  }
}

layer {
  name: "relu4x_5_2"
  type: "ReLU"
  bottom: "conv4x_5_2"
  top: "conv4x_5_2"
}
layer {
  name: "conv4x_5_3"
  type: "Convolution"
  bottom: "conv4x_5_2"
  top: "conv4x_5_3"
phase:PREDICT
  convolution_param {
    num_output: 1024
	kernel_size: 1
  }
}

layer{
name: "conv4x_5_sum"
type:  "Eltwise"
bottom:"conv4x_4_sum"
bottom:"conv4x_5_3"
top:"conv4x_5_sum"
eltwise_param{
operation: SUM
}
}

layer {
  name: "relu4x_5_sum"
  type: "ReLU"
  bottom: "conv4x_5_sum"
  top: "conv4x_5_sum"
  }
  
 #--------------------------block_layers  4x_6---------------- 
 layer {
  name: "conv4x_6_1"
  type: "Convolution"
  bottom: "conv4x_5_sum"
  top: "conv4x_6_1"
phase:PREDICT
  convolution_param {
    num_output: 256
    kernel_size: 1
  }
}

layer {
  name: "relu4x_6_1"
  type: "ReLU"
  bottom: "conv4x_6_1"
  top: "conv4x_6_1"
}
layer {
  name: "conv4x_6_2"
  type: "Convolution"
  bottom: "conv4x_6_1"
  top: "conv4x_6_2"
phase:PREDICT
  convolution_param {
    num_output: 256
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad:1
	pad:1
	pad:0
  }
}

layer {
  name: "relu4x_6_2"
  type: "ReLU"
  bottom: "conv4x_6_2"
  top: "conv4x_6_2"
}
layer {
  name: "conv4x_6_3"
  type: "Convolution"
  bottom: "conv4x_6_2"
  top: "conv4x_6_3"
phase:PREDICT
  convolution_param {
    num_output: 1024
	kernel_size: 1
  }
}

layer{
name: "conv4x_6_sum"
type:  "Eltwise"
bottom:"conv4x_5_sum"
bottom:"conv4x_6_3"
top:"conv4x_6_sum"
eltwise_param{
operation: SUM
}
}

layer {
  name: "relu4x_6_sum"
  type: "ReLU"
  bottom: "conv4x_6_sum"
  top: "conv4x_6_sum"
  }
  
  
 #----------------------conv5x----------------------------------------------------------------------------------------------------
 #---------------conv5_top------------------
 layer {
  name: "conv5x_top"
  type: "Convolution"
  bottom: "conv4x_6_sum"
  top: "conv5x_top"
phase:PREDICT
  convolution_param {
    num_output: 2048
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	stride :2
	stride :2
	stride :1
	pad :1
	pad :1
	pad :0
  }
 }
  
  
  
 #---------------conv5x_1------------------------------
layer {
  name: "conv5x_1_1"
  type: "Convolution"
  bottom: "conv5x_top"
  top: "conv5x_1_1"
phase:PREDICT
  convolution_param {
    num_output: 512
    kernel_size: 1
  }
}

layer {
  name: "relu5x_1_1"
  type: "ReLU"
  bottom: "conv5x_1_1"
  top: "conv5x_1_1"
}
layer {
  name: "conv5x_1_2"
  type: "Convolution"
  bottom: "conv5x_1_1"
  top: "conv5x_1_2"
phase:PREDICT
  convolution_param {
    num_output: 512
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad:1
	pad:1
	pad:0
  }
}

layer {
  name: "relu5x_1_2"
  type: "ReLU"
  bottom: "conv5x_1_2"
  top: "conv5x_1_2"
}
layer {
  name: "conv5x_1_3"
  type: "Convolution"
  bottom: "conv5x_1_2"
  top: "conv5x_1_3"
  convolution_param {
    num_output: 2048
	kernel_size: 1
  }
}

layer{
name: "conv5x_1_sum"
type:  "Eltwise"
bottom:"conv5x_top"
bottom:"conv5x_1_3"
top:"conv5x_1_sum"
eltwise_param{
operation: SUM
}
}

layer {
  name: "relu5x_1_sum"
  type: "ReLU"
  bottom: "conv5x_1_sum"
  top: "conv5x_1_sum"
  }
 #----------------------conv5x_2------------------------------
 layer {
  name: "conv5x_2_1"
  type: "Convolution"
  bottom: "conv5x_1_sum"
  top: "conv5x_2_1"
phase:PREDICT
  convolution_param {
    num_output: 512
    kernel_size: 1
  }
}

layer {
  name: "relu5x_2_1"
  type: "ReLU"
  bottom: "conv5x_2_1"
  top: "conv5x_2_1"
}
layer {
  name: "conv5x_2_2"
  type: "Convolution"
  bottom: "conv5x_2_1"
  top: "conv5x_2_2"
phase:PREDICT
  convolution_param {
    num_output: 512
    kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad:1
	pad:1
	pad:0
  }
}

layer {
  name: "relu5x_2_2"
  type: "ReLU"
  bottom: "conv5x_2_2"
  top: "conv5x_2_2"
}
layer {
  name: "conv5x_2_3"
  type: "Convolution"
  bottom: "conv5x_2_2"
  top: "conv5x_2_3"
phase:PREDICT
  convolution_param {
    num_output: 2048
	kernel_size: 1
  }
}

layer{
name: "conv5x_2_sum"
type:  "Eltwise"
bottom:"conv5x_1_sum"
bottom:"conv5x_2_3"
top:"conv5x_2_sum"
eltwise_param{
operation: SUM
}
}

layer {
  name: "relu5x_2_sum"
  type: "ReLU"
  bottom: "conv5x_2_sum"
  top: "conv5x_2_sum"
  }
 
 
 #---------------------------deconv--------------------------- 

layer{
name: "deconv1_1x"
  type: "Convolution"
  bottom: "conv2x_3_sum"
  top: "deconv1_1x"
  phase:PREDICT
  convolution_param {
    num_output: 128
    kernel_size: 1
	kernel_size: 1
	kernel_size: 1
	stride: 1
	stride: 1
	stride: 1
	pad:0
	pad:0
	pad:0
  }
}

layer{
name: "deconv2_1x"
  type: "Convolution"
  bottom: "conv3x_3_sum"
  top: "deconv2_1x"
  phase:PREDICT
  convolution_param {
    num_output: 128
    kernel_size: 1
	kernel_size: 1
	kernel_size: 1
	stride: 1
	stride: 1
	stride: 1
  }
}

layer{
name: "deconv3_1x"
  type: "Convolution"
  bottom: "conv4x_6_sum"
  top: "deconv3_1x"
  phase:PREDICT
  convolution_param {
    num_output: 128
    kernel_size: 1
	kernel_size: 1
	kernel_size: 1
	stride: 1
	stride: 1
	stride: 1
  }
}

layer{
name: "deconv4_1x"
  type: "Convolution"
  bottom: "conv5x_2_sum"
  top: "deconv4_1x"
  phase:PREDICT
  convolution_param {
    num_output: 128
    kernel_size: 1
	kernel_size: 1
	kernel_size: 1
	stride: 1
	stride: 1
	stride: 1
  }
}




layer{
name: "deconv4_2x"
  type: "Deconvolution"
  bottom: "deconv4_1x"
  top: "deconv4_2x"
phase:PREDICT
  convolution_param {
    num_output: 128
    kernel_size: 4
	kernel_size: 4
	kernel_size: 1
	stride: 2
	stride: 2
	stride: 1
	pad:1
	pad:1
	pad:0
   weight_filler {
      type: "bilinear"
    }
  }
}


layer{
name: "deconv_sum_43"
type:  "Eltwise"
bottom:"deconv4_2x"
bottom:"deconv3_1x"
top:"upsample43"
eltwise_param{
operation: SUM
}
}



layer{
name: "upsample43_2x"
  type: "Deconvolution"
  bottom: "upsample43"
  top: "upsample43_2x"
  phase:PREDICT
  convolution_param {
    num_output: 128
    kernel_size: 4
	kernel_size: 4
	kernel_size: 1
	stride: 2
	stride: 2
	stride: 1
	pad:1
	pad:1
	pad:0
	group: 2
   weight_filler {
      type: "bilinear"
    }
  }
}


layer{
name: "deconv_sum_432"
type:  "Eltwise"
bottom:"upsample43_2x"
bottom:"deconv2_1x"
top:"upsample432"
eltwise_param{
operation: SUM
}
}


layer{
name: "upsample432_2x"
  type: "Deconvolution"
  bottom: "upsample432"
  top: "upsample432_2x"
  phase:PREDICT
  convolution_param {
    num_output: 128
    kernel_size: 4
	kernel_size: 4
	kernel_size: 1
	stride: 2
	stride: 2
	stride: 1
	pad:1
	pad:1
	pad:0
	group: 2
   weight_filler {
      type: "bilinear"
    }
  }
}

layer{
name: "deconv_sum_4321"
type:  "Eltwise"
bottom:"upsample432_2x"
bottom:"deconv1_1x"
top:"upsample4321"
eltwise_param{
operation: SUM
}
}


layer{
name: "upsample4321_2x"
  type: "Deconvolution"
  bottom: "upsample4321"
  top: "upsample4321_2x"
  phase:PREDICT
  convolution_param {
    num_output: 128
    kernel_size: 8
	kernel_size: 8
	kernel_size: 1
	stride: 4
	stride: 4
	stride: 1
	pad:2
	pad:2
	pad:0
	group: 2
   weight_filler {
      type: "bilinear"
    }
  }
}


# layer{
# name: "deconv_sum"
# type:  "Eltwise"
# bottom:"deconv4_2x"
# bottom:"deconv3"
# bottom:"deconv3"
# bottom:"deconv4"
# top:"upsample"
# eltwise_param{
# operation: SUM
# }
# }



# layer{
# name: "deconv_sum"
# type:  "Eltwise"
# bottom:"deconv1"
# bottom:"deconv2"
# bottom:"deconv3"
# bottom:"deconv4"
# top:"upsample"
# eltwise_param{
# operation: SUM
# }
# }

layer {
  name: "fc8"
  type: "Convolution"
  bottom: "upsample4321_2x"
  top: "fc8"
  convolution_param {
    num_output: 128
	kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad: 1
	pad: 1
	pad: 0
  }
    phase:PREDICT
}

layer {
  name: "relu7-1"
  type: "ReLU"
  bottom: "fc8"
  top: "fc8"
}

layer {
  name: "fc8-2"
  type: "Convolution"
  bottom: "fc8"
  top: "fc8-2"
  convolution_param {
    num_output: 128
	kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad: 1
	pad: 1
	pad: 0
  }
    phase:PREDICT
}

layer {
  name: "relu7-2"
  type: "ReLU"
  bottom: "fc8-2"
  top: "fc8-2"
}

layer {
  name: "fc8-3"
  type: "Convolution"
  bottom: "fc8-2"
  top: "fc8-3"
  convolution_param {
    num_output: 128
	kernel_size: 3
	kernel_size: 3
	kernel_size: 1
	pad: 1
	pad: 1
	pad: 0
  }
    phase:PREDICT
}


layer{
name: "deconv_sum"
type:  "Eltwise"
bottom:"upsample4321_2x"
bottom:"fc8-3"
top:"fc8-4"
eltwise_param{
operation: SUM
}
}

layer {
   name: "relu8-3"
   type: "ReLU"
   bottom: "fc8-4"
   top: "fc8-4"
}



layer {
  name: "fc9"
  type: "Convolution"
  bottom: "fc8-4"
  top: "fc9"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
	kernel_size: 1
  }
  phase:PREDICT
}

#Deconvolution
#

layer {
  name: "output"
  type: "Softmax"
  bottom: "fc9"
  top: "output"
}
